{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# ðŸŽ™ï¸ Karakalpak VITS TTS Model Training on Google Colab\n\nThis notebook provides a complete pipeline for fine-tuning a VITS (Variational Inference Text-to-Speech) model for Karakalpak language using the MMS (Massively Multilingual Speech) architecture.\n\n## ðŸ“‹ Overview\n- **Model**: MMS-TTS (VITS architecture)\n- **Language**: Karakalpak (kaa)\n- **Dataset**: HuggingFace dataset `nickoo004/karakalpak-tts-speaker1`\n- **Training Time**: ~20-30 minutes on Colab GPU\n- **âœ… Works with Private Repositories** - No authentication needed!\n\n## ðŸš€ Steps:\n1. Environment Setup & Dependencies\n2. Download Repository (ZIP method - works with private repos!)\n3. Dataset Loading from HuggingFace\n4. Model Preparation\n5. Training Configuration\n6. Model Training\n7. Inference & Testing\n8. Model Saving & Upload"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1"
   },
   "source": [
    "## 1ï¸âƒ£ Environment Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"ðŸ” GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸ“Š GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: No GPU detected! Training will be very slow. Please enable GPU in Runtime > Change runtime type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2"
   },
   "source": [
    "## 2ï¸âƒ£ Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install core dependencies\n",
    "pip install -q transformers>=4.35.1 datasets[audio]>=2.14.7 accelerate>=0.24.1\n",
    "pip install -q matplotlib wandb tensorboard Cython\n",
    "pip install -q scipy librosa soundfile\n",
    "\n",
    "echo \"âœ… Dependencies installed successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section3"
   },
   "source": "## 3ï¸âƒ£ Download Repository & Build Monotonic Align\n\nThis will download the training code from GitHub (works with private repositories!)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": "import os\nimport zipfile\nimport urllib.request\nimport shutil\n\nrepo_name = \"my-vits-finetuner-karakalpak\"\n\n# IMPORTANT: Update this URL if your main branch is named differently\n# For 'main' branch: /archive/refs/heads/main.zip\n# For 'master' branch: /archive/refs/heads/master.zip\nzip_url = \"https://github.com/NursultanMRX/my-vits-finetuner-karakalpak/archive/refs/heads/main.zip\"\n\nif not os.path.exists(repo_name):\n    print(f\"ðŸ“¥ Downloading repository as ZIP (no authentication needed)...\")\n    zip_path = \"repo.zip\"\n    \n    try:\n        # Download the zip file\n        urllib.request.urlretrieve(zip_url, zip_path)\n        print(\"âœ… Downloaded successfully!\")\n        \n        # Extract the zip file\n        print(\"ðŸ“¦ Extracting files...\")\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(\".\")\n        \n        # The extracted folder has '-main' or '-master' suffix\n        # Try both possible names\n        extracted_name = None\n        for suffix in ['-main', '-master']:\n            possible_name = f\"{repo_name}{suffix}\"\n            if os.path.exists(possible_name):\n                extracted_name = possible_name\n                break\n        \n        if extracted_name:\n            # Rename to remove the branch suffix\n            shutil.move(extracted_name, repo_name)\n            print(f\"âœ… Renamed '{extracted_name}' to '{repo_name}'\")\n        else:\n            raise Exception(f\"Could not find extracted directory (expected {repo_name}-main or {repo_name}-master)\")\n        \n        # Clean up zip file\n        os.remove(zip_path)\n        print(\"âœ… Repository ready!\")\n        \n    except Exception as e:\n        print(f\"âŒ Error downloading repository: {e}\")\n        print(\"\\nâš ï¸ Troubleshooting:\")\n        print(\"1. Check if the repository exists and you have access\")\n        print(\"2. Verify the branch name (main or master) in the URL above\")\n        print(\"3. Try downloading manually and uploading to Colab\")\n        raise\nelse:\n    print(\"âœ… Repository already exists!\")\n\n# Change to repository directory\nif os.path.exists(repo_name) and os.path.isdir(repo_name):\n    os.chdir(repo_name)\n    print(f\"ðŸ“‚ Current directory: {os.getcwd()}\")\n    \n    # List some files to verify\n    files = os.listdir('.')\n    print(f\"\\nðŸ“‹ Repository contains {len(files)} items\")\n    print(f\"   Key files present: {', '.join([f for f in ['run_vits_finetuning.py', 'monotonic_align', 'utils'] if f in files])}\")\nelse:\n    raise Exception(f\"âŒ Directory '{repo_name}' does not exist!\")"
  },
  {
   "cell_type": "code",
   "source": "# Alternative: Use Git Clone (only works for PUBLIC repos or with authentication)\n# Uncomment and use this instead if you prefer git clone\n\n# import subprocess\n# repo_url = \"https://github.com/NursultanMRX/my-vits-finetuner-karakalpak.git\"\n# repo_name = \"my-vits-finetuner-karakalpak\"\n# \n# if not os.path.exists(repo_name):\n#     result = subprocess.run(['git', 'clone', repo_url], capture_output=True, text=True)\n#     if result.returncode != 0:\n#         print(f\"Git clone failed: {result.stderr}\")\n#     else:\n#         print(\"âœ… Repository cloned successfully!\")\n# \n# os.chdir(repo_name)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "build_monotonic"
   },
   "outputs": [],
   "source": "%%bash\n# Build the Cython monotonic alignment module (CRITICAL for fast training)\necho \"ðŸ”¨ Building monotonic alignment module...\"\n\n# Check if we're in the right directory\nif [ ! -d \"monotonic_align\" ]; then\n    echo \"âŒ Error: monotonic_align directory not found!\"\n    echo \"Current directory: $(pwd)\"\n    echo \"Please make sure the repository was cloned correctly.\"\n    exit 1\nfi\n\ncd monotonic_align\n\n# Create the monotonic_align subdirectory if it doesn't exist\nmkdir -p monotonic_align\n\n# Build the Cython extension\npython setup.py build_ext --inplace\n\nif [ $? -eq 0 ]; then\n    echo \"âœ… Monotonic align built successfully!\"\nelse\n    echo \"âŒ Failed to build monotonic align!\"\n    exit 1\nfi\n\ncd .."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4"
   },
   "source": [
    "## 4ï¸âƒ£ HuggingFace Authentication (Optional but Recommended)\n",
    "\n",
    "**Note**: This is required if you want to:\n",
    "- Push your trained model to HuggingFace Hub\n",
    "- Access private datasets\n",
    "\n",
    "Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf_login"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login to HuggingFace (you'll be prompted for your token)\n",
    "# If you don't want to push to hub, you can skip this step\n",
    "try:\n",
    "    notebook_login()\n",
    "    print(\"âœ… Successfully logged in to HuggingFace!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Login skipped or failed: {e}\")\n",
    "    print(\"You can continue training, but won't be able to push to Hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section5"
   },
   "source": [
    "## 5ï¸âƒ£ Load and Explore Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Karakalpak TTS dataset from HuggingFace\n",
    "dataset_name = \"nickoo004/karakalpak-tts-speaker1\"\n",
    "\n",
    "print(f\"ðŸ“Š Loading dataset: {dataset_name}\")\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "print(\"\\nâœ… Dataset loaded successfully!\")\n",
    "print(f\"\\nðŸ“ˆ Dataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Show a sample from the dataset\n",
    "if 'train' in dataset:\n",
    "    sample = dataset['train'][0]\n",
    "    print(\"\\nðŸ” Sample from dataset:\")\n",
    "    print(f\"  - Text: {sample.get('text', 'N/A')}\")\n",
    "    print(f\"  - Audio file: {sample.get('audio_file', 'N/A')}\")\n",
    "    print(f\"  - Speaker: {sample.get('speaker_name', 'N/A')}\")\n",
    "    print(f\"\\nðŸ“Š Total training samples: {len(dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section6"
   },
   "source": [
    "## 6ï¸âƒ£ Prepare or Download the Base Model with Discriminator\n",
    "\n",
    "For Karakalpak (kaa), we need to either:\n",
    "- Use an existing checkpoint with discriminator (if available)\n",
    "- Convert the original MMS checkpoint to include discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_model"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "\n",
    "# Configuration\n",
    "model_name_or_path = \"facebook/mms-tts-kaa\"  # Base MMS model for Karakalpak\n",
    "local_model_dir = \"./mms-tts-kaa-with-discriminator\"\n",
    "\n",
    "# Check if we already have a model with discriminator on HuggingFace\n",
    "# You can replace this with your own model if you've already created one\n",
    "try:\n",
    "    # Try to check if a pre-converted model exists\n",
    "    test_model = \"nickoo004/mms-tts-kaa-with-discriminator\"\n",
    "    files = list_repo_files(test_model)\n",
    "    if 'discriminator.pth' in files or 'pytorch_model.bin' in files:\n",
    "        print(f\"âœ… Found existing model with discriminator: {test_model}\")\n",
    "        model_name_or_path = test_model\n",
    "    else:\n",
    "        print(f\"âš ï¸ Model exists but discriminator not found. Will convert.\")\n",
    "        raise Exception(\"Need to convert\")\n",
    "except:\n",
    "    print(f\"\\nðŸ“ No pre-converted model found. Converting base MMS model...\")\n",
    "    print(f\"   Base model: {model_name_or_path}\")\n",
    "    \n",
    "    # Convert the model to include discriminator\n",
    "    # This only needs to be done ONCE per language\n",
    "    !python convert_original_discriminator_checkpoint.py \\\n",
    "        --language_code kaa \\\n",
    "        --pytorch_dump_folder_path {local_model_dir}\n",
    "    \n",
    "    model_name_or_path = local_model_dir\n",
    "    print(f\"\\nâœ… Model converted and saved to: {model_name_or_path}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Model ready for training: {model_name_or_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section7"
   },
   "source": [
    "## 7ï¸âƒ£ Configure Training Parameters\n",
    "\n",
    "These settings are optimized for fine-tuning on a single speaker dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Training configuration based on finetune_karakalpak.json\n",
    "# Adjust these parameters as needed for your use case\n",
    "\n",
    "training_config = {\n",
    "    # Model and Dataset\n",
    "    \"model_name_or_path\": model_name_or_path,\n",
    "    \"dataset_name\": \"nickoo004/karakalpak-tts-speaker1\",\n",
    "    \n",
    "    # Output directories\n",
    "    \"output_dir\": \"./mms-tts-kaa-finetuned-speaker1\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \n",
    "    # HuggingFace Hub (set to False if you don't want to push)\n",
    "    \"push_to_hub\": False,  # Set to True if you want to upload to Hub\n",
    "    \"hub_model_id\": \"your-username/mms-tts-kaa-finetuned-speaker1\",  # Change this!\n",
    "    \n",
    "    # Dataset columns\n",
    "    \"audio_column_name\": \"audio_file\",\n",
    "    \"text_column_name\": \"text\",\n",
    "    \"speaker_id_column_name\": \"speaker_name\",\n",
    "    \"filter_on_speaker_id\": \"Speaker_1\",  # Filter to use only Speaker_1\n",
    "    \"override_speaker_embeddings\": True,\n",
    "    \n",
    "    # Audio filtering\n",
    "    \"max_duration_in_seconds\": 20.0,\n",
    "    \"min_duration_in_seconds\": 1.0,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"num_train_epochs\": 150,  # Reduce for faster testing (e.g., 50)\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"warmup_ratio\": 0.01,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_checkpointing\": False,\n",
    "    \"group_by_length\": False,\n",
    "    \n",
    "    # Training flags\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": False,\n",
    "    \n",
    "    # Loss weights (GAN training)\n",
    "    \"weight_disc\": 3.0,\n",
    "    \"weight_fmaps\": 1.0,\n",
    "    \"weight_gen\": 1.0,\n",
    "    \"weight_kl\": 1.5,\n",
    "    \"weight_duration\": 1.0,\n",
    "    \"weight_mel\": 35.0,\n",
    "    \n",
    "    # Optimization\n",
    "    \"fp16\": True,  # Use mixed precision for faster training\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # Logging and saving\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 500,\n",
    "    \"save_total_limit\": 2,\n",
    "}\n",
    "\n",
    "# Save config to file\n",
    "config_path = \"./training_config_colab.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(training_config, f, indent=4)\n",
    "\n",
    "print(\"âœ… Training configuration created!\")\n",
    "print(\"\\nðŸ“‹ Key settings:\")\n",
    "print(f\"  - Model: {training_config['model_name_or_path']}\")\n",
    "print(f\"  - Dataset: {training_config['dataset_name']}\")\n",
    "print(f\"  - Epochs: {training_config['num_train_epochs']}\")\n",
    "print(f\"  - Batch size: {training_config['per_device_train_batch_size']}\")\n",
    "print(f\"  - Learning rate: {training_config['learning_rate']}\")\n",
    "print(f\"  - Output: {training_config['output_dir']}\")\n",
    "print(f\"  - Push to Hub: {training_config['push_to_hub']}\")\n",
    "print(f\"\\nðŸ“„ Config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section8"
   },
   "source": [
    "## 8ï¸âƒ£ Start Training! ðŸš€\n",
    "\n",
    "This will fine-tune the VITS model on your Karakalpak dataset.\n",
    "\n",
    "**Note**: Training will take approximately 20-30 minutes on a GPU (T4 or better).\n",
    "\n",
    "If you encounter memory issues, reduce `per_device_train_batch_size` to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "# Run training using accelerate\n",
    "!accelerate launch run_vits_finetuning.py {config_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section9"
   },
   "source": [
    "## 9ï¸âƒ£ Test the Trained Model - Inference\n",
    "\n",
    "Let's generate some speech with your fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_setup"
   },
   "outputs": [],
   "source": [
    "from transformers import VitsModel, AutoTokenizer\n",
    "import torch\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = training_config['output_dir']\n",
    "\n",
    "print(f\"ðŸ“¥ Loading model from: {model_path}\")\n",
    "model = VitsModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ… Model loaded successfully on {device}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_test"
   },
   "outputs": [],
   "source": [
    "# Test with Karakalpak text samples\n",
    "test_texts = [\n",
    "    \"SÃ¡lem, qalaysÄ±z?\",  # Hello, how are you?\n",
    "    \"MÃºgÃ¡lim jÃ¡qsÄ±.\",    # The teacher is good.\n",
    "    \"Men oqÄ±p atÄ±rman.\",  # I am studying.\n",
    "]\n",
    "\n",
    "print(\"ðŸŽ¤ Generating speech samples...\\n\")\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sample {i}: {text}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate speech\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get waveform\n",
    "    waveform = outputs.waveform[0].cpu().numpy()\n",
    "    \n",
    "    # Save audio file\n",
    "    output_file = f\"output_sample_{i}.wav\"\n",
    "    scipy.io.wavfile.write(\n",
    "        output_file,\n",
    "        rate=model.config.sampling_rate,\n",
    "        data=waveform\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸ’¾ Saved to: {output_file}\")\n",
    "    \n",
    "    # Play audio in notebook\n",
    "    display(Audio(waveform, rate=model.config.sampling_rate))\n",
    "\n",
    "print(\"\\nâœ… All samples generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section10"
   },
   "source": [
    "## ðŸ”Ÿ Custom Text Generation\n",
    "\n",
    "Try your own Karakalpak text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_inference"
   },
   "outputs": [],
   "source": [
    "# Enter your custom Karakalpak text here\n",
    "custom_text = \"SÃ¡lem, bul men!\"  # Change this to your text\n",
    "\n",
    "print(f\"ðŸ“ Generating speech for: {custom_text}\\n\")\n",
    "\n",
    "inputs = tokenizer(custom_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "waveform = outputs.waveform[0].cpu().numpy()\n",
    "\n",
    "# Save\n",
    "output_file = \"custom_output.wav\"\n",
    "scipy.io.wavfile.write(\n",
    "    output_file,\n",
    "    rate=model.config.sampling_rate,\n",
    "    data=waveform\n",
    ")\n",
    "\n",
    "print(f\"âœ… Generated and saved to: {output_file}\\n\")\n",
    "\n",
    "# Play\n",
    "display(Audio(waveform, rate=model.config.sampling_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section11"
   },
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ Push Model to HuggingFace Hub (Optional)\n",
    "\n",
    "Share your model with the community!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "push_hub"
   },
   "outputs": [],
   "source": [
    "# Only run this if you want to upload your model to HuggingFace Hub\n",
    "# Make sure you've logged in earlier and set a proper hub_model_id\n",
    "\n",
    "push_to_hub = False  # Set to True to enable pushing\n",
    "\n",
    "if push_to_hub:\n",
    "    hub_model_id = training_config['hub_model_id']\n",
    "    \n",
    "    if hub_model_id and \"your-username\" not in hub_model_id:\n",
    "        print(f\"ðŸ“¤ Pushing model to HuggingFace Hub: {hub_model_id}\")\n",
    "        \n",
    "        model.push_to_hub(hub_model_id)\n",
    "        tokenizer.push_to_hub(hub_model_id)\n",
    "        \n",
    "        print(f\"\\nâœ… Model successfully pushed to: https://huggingface.co/{hub_model_id}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Please set a valid hub_model_id (replace 'your-username' with your HF username)\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Skipping push to Hub (set push_to_hub=True to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section12"
   },
   "source": [
    "## 1ï¸âƒ£2ï¸âƒ£ Download Trained Model Files\n",
    "\n",
    "Download your trained model to use locally or deploy elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "# Create a zip file of the trained model\n",
    "import shutil\n",
    "\n",
    "output_dir = training_config['output_dir']\n",
    "zip_filename = \"mms-tts-kaa-finetuned\"\n",
    "\n",
    "print(f\"ðŸ“¦ Creating zip archive of trained model...\")\n",
    "shutil.make_archive(zip_filename, 'zip', output_dir)\n",
    "\n",
    "print(f\"\\nâœ… Model archived to: {zip_filename}.zip\")\n",
    "print(f\"ðŸ“¥ You can download it from the Files panel on the left\")\n",
    "print(f\"   Size: {os.path.getsize(f'{zip_filename}.zip') / (1024*1024):.2f} MB\")\n",
    "\n",
    "# List files in the output directory\n",
    "print(f\"\\nðŸ“‚ Model files:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path) / (1024*1024)\n",
    "        print(f\"  - {file}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section13"
   },
   "source": [
    "## ðŸ“Š Training Summary & Next Steps\n",
    "\n",
    "### What we accomplished:\n",
    "âœ… Set up the training environment\n",
    "âœ… Loaded the Karakalpak dataset from HuggingFace\n",
    "âœ… Prepared/converted the MMS-TTS model with discriminator\n",
    "âœ… Fine-tuned the model on Karakalpak speech data\n",
    "âœ… Generated test audio samples\n",
    "âœ… Saved the trained model\n",
    "\n",
    "### Next Steps:\n",
    "1. **Experiment with hyperparameters**: Try different learning rates, batch sizes, or epochs\n",
    "2. **Use more data**: Add more speakers or audio samples to improve quality\n",
    "3. **Deploy**: Use the model in a web app or mobile application\n",
    "4. **Share**: Upload to HuggingFace Hub to share with the community\n",
    "\n",
    "### Useful Resources:\n",
    "- [VITS Paper](https://arxiv.org/abs/2106.06103)\n",
    "- [MMS Paper](https://arxiv.org/abs/2305.13516)\n",
    "- [HuggingFace VITS Docs](https://huggingface.co/docs/transformers/model_doc/vits)\n",
    "- [Transformers TTS Guide](https://huggingface.co/docs/transformers/tasks/text-to-speech)\n",
    "\n",
    "### ðŸŽ‰ Congratulations on training your Karakalpak TTS model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": "## ðŸ”§ Troubleshooting\n\n### Common Issues:\n\n**1. Repository Download Fails:**\n- Check if you have internet connection\n- Verify the repository exists: https://github.com/NursultanMRX/my-vits-finetuner-karakalpak\n- If branch name is different, update the `zip_url` (change 'main' to 'master' if needed)\n- As a last resort, manually download the ZIP and upload to Colab Files panel\n\n**2. Out of Memory (OOM) Error:**\n- Reduce `per_device_train_batch_size` to 2 or 1\n- Enable `gradient_checkpointing: true`\n- Ensure you're using a GPU runtime (Runtime > Change runtime type)\n\n**3. Training is slow:**\n- Make sure GPU is enabled (should see T4/V100/A100 in GPU check)\n- Ensure `fp16: true` is set\n- Check that monotonic_align was built correctly\n\n**4. Model quality is poor:**\n- Increase `num_train_epochs` (try 200-300)\n- Check dataset quality and text alignment\n- Ensure audio samples are clean and well-recorded\n- Try adjusting loss weights (especially `weight_mel`)\n\n**5. Dataset loading fails:**\n- Verify dataset exists: https://huggingface.co/datasets/nickoo004/karakalpak-tts-speaker1\n- Check internet connection\n- Try restarting the runtime\n\n**6. Can't push to Hub:**\n- Make sure you ran `notebook_login()` successfully\n- Verify your HuggingFace token has write permissions\n- Check that `hub_model_id` doesn't have 'your-username' in it\n\n**7. Monotonic Align Build Fails:**\n- Make sure Cython is installed: `!pip install Cython`\n- Check that numpy is installed: `!pip install numpy`\n- Try restarting runtime and running from the beginning"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}